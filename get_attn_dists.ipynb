{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from fairseq import (\n",
    "    checkpoint_utils,\n",
    "    distributed_utils,\n",
    "    options,\n",
    "    quantization_utils,\n",
    "    tasks,\n",
    "    utils,\n",
    ")\n",
    "from fairseq.data import iterators\n",
    "from fairseq.logging import meters, metrics, progress_bar\n",
    "from fairseq.trainer import Trainer\n",
    "from fairseq.model_parallel.megatron_trainer import MegatronTrainer\n",
    "from fairseq.models.pruned_transformer import PrunedTransformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = pickle.load(open(\"argsfile.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.data = '/raj-learn/data/wmt16_en_de_bpe32k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = tasks.setup_task(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.load_dataset(args.valid_subset, combine=False, epoch=1)\n",
    "dataset = task.dataset(args.valid_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_dir = \"/home/raj/data/raj-learn/checkpoints/lr-rewind_0.75sparsity_0.2frac_30epochs/\"\n",
    "checkpoint_dir = \"/raj-learn/checkpoints/lr-rewind_0.75sparsity_0.2frac_30epochs/\"\n",
    "model_paths = [\"checkpoint_LTH0_epoch60.pt\",\n",
    "              \"checkpoint_LTH1_epoch60_sparsity0.168.pt\",\n",
    "              \"checkpoint_LTH2_epoch60_sparsity0.302.pt\",\n",
    "              \"checkpoint_LTH3_epoch60_sparsity0.410.pt\", \n",
    "              \"checkpoint_LTH4_epoch60_sparsity0.496.pt\", \n",
    "              \"checkpoint_LTH5_epoch60_sparsity0.565.pt\",\n",
    "              \"checkpoint_LTH6_epoch60_sparsity0.620.pt\",\n",
    "              \"checkpoint_LTH7_epoch60_sparsity0.664.pt\",\n",
    "              \"checkpoint_LTH8_epoch60_sparsity0.699.pt\",\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def make_hdf5_file(vectors, output_file_path):\n",
    "    '''\n",
    "    Vectors: int -> np.array\n",
    "    Creates hdf5 file.\n",
    "    '''\n",
    "    with h5py.File(output_file_path, 'w') as fout:\n",
    "        for key, embeddings in vectors.items():\n",
    "            fout.create_dataset(\n",
    "                str(key),\n",
    "                embeddings.shape, dtype='float32',\n",
    "                data=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:15<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model LTH0 took 172.75sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:15<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model LTH1 took 209.46sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:14<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model LTH2 took 202.57sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:16<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model LTH3 took 207.36sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:15<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model LTH4 took 213.00sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:24<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model LTH5 took 228.70sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:30<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model LTH6 took 229.51sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:31<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model LTH7 took 228.82sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:29<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model LTH8 took 213.13sec\n",
      "CPU times: user 9min 16s, sys: 6min 35s, total: 15min 52s\n",
      "Wall time: 31min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "for path in model_paths:\n",
    "    t0 = time.time()\n",
    "    model_name = path.split('_')[1]\n",
    "    \n",
    "    args.path = checkpoint_dir + path\n",
    "    models, _model_args = checkpoint_utils.load_model_ensemble(\n",
    "        args.path.split(os.pathsep),\n",
    "        task=task,\n",
    "    )\n",
    "    model = models[0]\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    itr = task.get_batch_iterator(\n",
    "            dataset=dataset,\n",
    "            max_tokens=args.max_tokens,\n",
    "            max_sentences=args.max_sentences,\n",
    "            max_positions=utils.resolve_max_positions(\n",
    "                task.max_positions(),\n",
    "                *[m.max_positions() for m in models],\n",
    "            ),\n",
    "            ignore_invalid_inputs=args.skip_invalid_size_inputs_valid_test,\n",
    "            required_batch_size_multiple=args.required_batch_size_multiple,\n",
    "            seed=args.seed,\n",
    "            num_shards=args.distributed_world_size,\n",
    "            shard_id=args.distributed_rank,\n",
    "            num_workers=args.num_workers,\n",
    "        ).next_epoch_itr(shuffle=False)\n",
    "    \n",
    "    all_attns_encenc = {}\n",
    "    all_attns_encdec = {}\n",
    "    all_attns_decdec = {}\n",
    "    for batch in tqdm(itr):\n",
    "        ids = batch[\"id\"].cpu().numpy().tolist()\n",
    "        src_lens = batch[\"net_input\"][\"src_lengths\"].cpu().numpy()\n",
    "        enc_outputs = model.encoder(batch[\"net_input\"][\"src_tokens\"].cuda(), batch[\"net_input\"][\"src_lengths\"].cuda(), \n",
    "                                    return_all_hiddens=False, return_all_attns=True)\n",
    "        encenc_attns = np.array([x.detach().cpu().numpy() for x in enc_outputs.encoder_self_attns])\n",
    "\n",
    "        out, props = model(batch[\"net_input\"][\"src_tokens\"].cuda(), batch[\"net_input\"][\"src_lengths\"].cuda(), \\\n",
    "                            batch[\"net_input\"][\"prev_output_tokens\"].cuda())\n",
    "        encdec_attns = [x.detach().cpu().numpy() for x in props[\"encdec_attns\"]]\n",
    "        decdec_attns = [x.detach().cpu().numpy() for x in props[\"decdec_attns\"]]\n",
    "\n",
    "        pad_lens = torch.sum(batch['target'] == 1, axis=1)\n",
    "        tgt_lens = batch['target'].shape[1] - pad_lens\n",
    "\n",
    "        for i, id_ in enumerate(ids):\n",
    "            all_attns_encenc[id_] = np.array([attn[i, :, -src_lens[i]:, -src_lens[i]:] for attn in encenc_attns])\n",
    "            all_attns_encdec[id_] = np.array([attn[:, i, :tgt_lens[i], -src_lens[i]:] for attn in encdec_attns])\n",
    "            all_attns_decdec[id_] = np.array([attn[i, :, :tgt_lens[i], :tgt_lens[i]] for attn in decdec_attns])\n",
    "    for (j, attntype) in enumerate([all_attns_encenc, all_attns_encdec, all_attns_decdec]):\n",
    "        if j == 0:\n",
    "            outfile = f'/raj-learn/data/precomputed_attns/{model_name}/encenc_attns_wmt_en_de_val.hdf5'\n",
    "        elif j == 1:\n",
    "            outfile = f'/raj-learn/data/precomputed_attns/{model_name}/encdec_attns_wmt_en_de_val.hdf5'\n",
    "        else:\n",
    "            outfile = f'/raj-learn/data/precomputed_attns/{model_name}/decdec_attns_wmt_en_de_val.hdf5'\n",
    "        make_hdf5_file(attntype, outfile)\n",
    "    print(\"Model %s took %.2fsec\" % (model_name, time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id_, mask in all_masks.items():\n",
    "    assert np.allclose(1, np.sum(mask[\"encenc\"][5][0,1,:]))\n",
    "    assert np.allclose(1, np.sum(mask[\"encdec\"][-1][0,1,:]))\n",
    "    assert np.allclose(1, np.sum(mask[\"decdec\"][-1][0,1,:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
